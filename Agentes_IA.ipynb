{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASVpw-Y6MxZf"
   },
   "source": [
    "# Aula 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Nota: \n",
    "$ uv init google-colab\n",
    "$ cd google-colab/\n",
    "$ uv venv\n",
    "$ source .venv/bin/activate\n",
    "$ code .\n",
    "$ uv pip install langchain-google-genai google-generativeai\n",
    "$ uv pip install ipykernel\n",
    "$ uv pip install jupyter\n",
    "$ uv pip install python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Acessa a chave da vari√°vel de ambiente\n",
    "gemini_key = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1757624376075,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "bX8cUIvL81HX"
   },
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature = 0,          # Criatividade das respostas (0 a 1) 0 √© mais preciso\n",
    "    api_key=gemini_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21180,
     "status": "ok",
     "timestamp": 1757624703449,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "3ed42Ob5jrhf",
    "outputId": "26ee31bb-07b0-4322-f3f4-ec89321ba957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Retrieval Augmented Generation) √© uma t√©cnica poderosa que combina a capacidade de gera√ß√£o de texto de um Large Language Model (LLM) com a capacidade de recuperar informa√ß√µes de uma base de conhecimento externa. Em termos simples, ele permite que um LLM \"consulte\" documentos espec√≠ficos antes de responder a uma pergunta, em vez de depender apenas do conhecimento em que foi treinado.\n",
      "\n",
      "Vamos detalhar o que voc√™ precisa saber e como us√°-lo.\n",
      "\n",
      "---\n",
      "\n",
      "### O que √© RAG (Retrieval Augmented Generation)?\n",
      "\n",
      "**R**etrieval (Recupera√ß√£o): O sistema busca informa√ß√µes relevantes em uma base de dados externa (seus documentos, artigos, FAQs, etc.).\n",
      "**A**ugmented (Aumentada): As informa√ß√µes recuperadas s√£o adicionadas √† sua pergunta original, \"aumentando\" o prompt.\n",
      "**G**eneration (Gera√ß√£o): O LLM usa este prompt aumentado (sua pergunta + contexto recuperado) para gerar uma resposta mais precisa e fundamentada.\n",
      "\n",
      "**Por que RAG √© importante?**\n",
      "\n",
      "1.  **Reduz Alucina√ß√µes:** LLMs podem \"inventar\" fatos. RAG os for√ßa a basear suas respostas em dados reais.\n",
      "2.  **Informa√ß√£o Atualizada:** LLMs s√£o treinados at√© uma data espec√≠fica. RAG permite que eles acessem informa√ß√µes mais recentes ou propriet√°rias.\n",
      "3.  **Conhecimento Espec√≠fico/Privado:** Voc√™ pode usar seus pr√≥prios documentos internos, manuais, bases de dados, etc., que o LLM nunca viu.\n",
      "4.  **Transpar√™ncia e Atribui√ß√£o:** Muitas implementa√ß√µes de RAG podem mostrar as fontes de onde a informa√ß√£o foi recuperada.\n",
      "5.  **Custo-benef√≠cio:** √â mais barato e r√°pido do que fazer fine-tuning de um LLM para cada novo conjunto de dados.\n",
      "\n",
      "---\n",
      "\n",
      "### Como o RAG Funciona (Passo a Passo Conceitual)\n",
      "\n",
      "Imagine que voc√™ tem uma biblioteca de livros e um assistente muito inteligente (o LLM), mas que s√≥ leu alguns livros.\n",
      "\n",
      "1.  **Prepara√ß√£o da Biblioteca (Indexa√ß√£o dos Dados):**\n",
      "    *   **Seus Documentos:** Voc√™ tem v√°rios documentos (PDFs, Word, texto puro, p√°ginas web, etc.).\n",
      "    *   **Quebra em Peda√ßos (Chunking):** Cada documento √© dividido em pequenos \"peda√ßos\" ou \"chunks\" (par√°grafos, se√ß√µes, frases). Isso √© crucial porque o LLM tem um limite de tokens que pode processar de uma vez.\n",
      "    *   **Vetoriza√ß√£o (Embedding):** Cada \"chunk\" √© transformado em um vetor num√©rico (uma lista de n√∫meros) por um \"modelo de embedding\". Vetores semanticamente semelhantes (que significam coisas parecidas) ficam \"pr√≥ximos\" no espa√ßo vetorial.\n",
      "    *   **Banco de Dados Vetorial (Vector Database):** Esses vetores s√£o armazenados em um banco de dados especial (como Pinecone, Weaviate, Chroma, FAISS, etc.), junto com o texto original do chunk.\n",
      "\n",
      "2.  **A Consulta (Quando o Usu√°rio Pergunta):**\n",
      "    *   **Pergunta do Usu√°rio:** Voc√™ faz uma pergunta (ex: \"Qual √© a pol√≠tica de f√©rias da empresa?\").\n",
      "    *   **Vetoriza√ß√£o da Pergunta:** Sua pergunta tamb√©m √© transformada em um vetor pelo mesmo modelo de embedding.\n",
      "    *   **Busca por Similaridade:** O sistema compara o vetor da sua pergunta com todos os vetores no banco de dados vetorial para encontrar os \"chunks\" mais semanticamente semelhantes.\n",
      "    *   **Recupera√ß√£o dos Chunks:** Os chunks mais relevantes (e seus textos originais) s√£o recuperados.\n",
      "\n",
      "3.  **Gera√ß√£o da Resposta (LLM em A√ß√£o):**\n",
      "    *   **Constru√ß√£o do Prompt:** Os chunks recuperados s√£o inseridos em um prompt junto com a sua pergunta original. Exemplo:\n",
      "        ```\n",
      "        \"Contexto:\n",
      "        [Chunk 1: 'A pol√≠tica de f√©rias da empresa X permite 30 dias de f√©rias anuais ap√≥s 1 ano de servi√ßo...']\n",
      "        [Chunk 2: 'As f√©rias devem ser solicitadas com 60 dias de anteced√™ncia...']\n",
      "\n",
      "        Pergunta: Qual √© a pol√≠tica de f√©rias da empresa?\n",
      "        ```\n",
      "    *   **LLM Gera a Resposta:** O LLM recebe este prompt enriquecido e usa o contexto fornecido para gerar uma resposta precisa e relevante.\n",
      "\n",
      "---\n",
      "\n",
      "### O que Voc√™ Precisa Saber/Ter para Implementar RAG?\n",
      "\n",
      "1.  **Seus Dados (Base de Conhecimento):**\n",
      "    *   **Formato:** PDFs, TXT, DOCX, HTML, JSON, CSV, etc.\n",
      "    *   **Qualidade:** Dados limpos, bem estruturados e relevantes s√£o cruciais. \"Garbage in, garbage out\" se aplica aqui.\n",
      "\n",
      "2.  **Ferramentas de Processamento de Texto:**\n",
      "    *   **Bibliotecas de Chunking:** Para dividir seus documentos em peda√ßos gerenci√°veis. Exemplos: `LangChain`, `LlamaIndex` (ambos em Python) oferecem utilit√°rios para isso.\n",
      "    *   **Loaders de Documentos:** Para ler diferentes formatos de arquivo (PDF, DOCX, etc.). Tamb√©m oferecidos por `LangChain` e `LlamaIndex`.\n",
      "\n",
      "3.  **Modelos de Embedding:**\n",
      "    *   **Fun√ß√£o:** Transformar texto em vetores num√©ricos.\n",
      "    *   **Onde encontrar:** OpenAI (text-embedding-ada-002, text-embedding-3-small/large), Cohere, Hugging Face (ex: `sentence-transformers`).\n",
      "    *   **Escolha:** Modelos maiores geralmente s√£o melhores, mas mais caros/lentos.\n",
      "\n",
      "4.  **Banco de Dados Vetorial (Vector Database):**\n",
      "    *   **Fun√ß√£o:** Armazenar e pesquisar eficientemente os vetores dos seus chunks.\n",
      "    *   **Op√ß√µes:**\n",
      "        *   **Gerenciados (Cloud):** Pinecone, Weaviate, Qdrant, Milvus (tamb√©m pode ser self-hosted).\n",
      "        *   **Locais/Em Mem√≥ria:** ChromaDB, FAISS (para prototipagem ou datasets menores).\n",
      "    *   **Escolha:** Depende da escala, custo, facilidade de uso e recursos necess√°rios.\n",
      "\n",
      "5.  **Large Language Model (LLM):**\n",
      "    *   **Fun√ß√£o:** Gerar a resposta final.\n",
      "    *   **Op√ß√µes:**\n",
      "        *   **APIs (Cloud):** OpenAI (GPT-3.5, GPT-4), Anthropic (Claude), Google (Gemini).\n",
      "        *   **Open Source (Self-hosted):** Llama 2, Mistral, Mixtral (exigem hardware potente para rodar localmente).\n",
      "    *   **Escolha:** Depende do custo, desempenho, privacidade e capacidade de hardware.\n",
      "\n",
      "6.  **Frameworks de Orquestra√ß√£o (Opcional, mas Altamente Recomendado):**\n",
      "    *   **LangChain (Python/JS):** Um dos mais populares. Facilita a constru√ß√£o de pipelines complexos, integrando loaders, chunkers, embeddings, vector dbs e LLMs.\n",
      "    *   **LlamaIndex (Python):** Focado especificamente em RAG e indexa√ß√£o de dados para LLMs. √ìtimo para come√ßar com RAG.\n",
      "    *   **Outros:** Semantic Kernel (Microsoft, .NET), Haystack (Deepset).\n",
      "\n",
      "7.  **Conhecimento de Programa√ß√£o:**\n",
      "    *   **Python:** √â a linguagem dominante para IA e ML, e a maioria das bibliotecas e frameworks est√£o em Python.\n",
      "\n",
      "---\n",
      "\n",
      "### Como Usar RAG (Passos Pr√°ticos de Implementa√ß√£o)\n",
      "\n",
      "Vamos usar um exemplo com Python, LangChain e ChromaDB (para simplicidade).\n",
      "\n",
      "**Fase 1: Indexa√ß√£o (Uma Vez ou Periodicamente)**\n",
      "\n",
      "1.  **Instale as Bibliotecas:**\n",
      "    ```bash\n",
      "    pip install langchain openai chromadb pypdf\n",
      "    ```\n",
      "2.  **Carregue Seus Documentos:**\n",
      "    ```python\n",
      "    from langchain_community.document_loaders import PyPDFLoader\n",
      "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
      "    from langchain_openai import OpenAIEmbeddings\n",
      "    from langchain_community.vectorstores import Chroma\n",
      "    import os\n",
      "\n",
      "    # Configure sua chave da OpenAI\n",
      "    os.environ[\"OPENAI_API_KEY\"] = \"sua_chave_openai_aqui\"\n",
      "\n",
      "    # 1. Carregar Documentos (ex: um PDF)\n",
      "    loader = PyPDFLoader(\"seus_documentos/politica_da_empresa.pdf\")\n",
      "    documents = loader.load()\n",
      "\n",
      "    # 2. Dividir em Chunks\n",
      "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
      "    chunks = text_splitter.split_documents(documents)\n",
      "\n",
      "    # 3. Gerar Embeddings\n",
      "    embeddings = OpenAIEmbeddings()\n",
      "\n",
      "    # 4. Armazenar no Vector Database (ChromaDB neste exemplo)\n",
      "    # Isso cria um diret√≥rio 'chroma_db' para armazenar os embeddings\n",
      "    vector_db = Chroma.from_documents(\n",
      "        documents=chunks,\n",
      "        embedding=embeddings,\n",
      "        persist_directory=\"./chroma_db\"\n",
      "    )\n",
      "    vector_db.persist() # Salva o banco de dados no disco\n",
      "    print(\"Documentos indexados com sucesso!\")\n",
      "    ```\n",
      "\n",
      "**Fase 2: Consulta (Quando o Usu√°rio Faz uma Pergunta)**\n",
      "\n",
      "1.  **Carregue o Vector Database e o LLM:**\n",
      "    ```python\n",
      "    from langchain_openai import ChatOpenAI\n",
      "    from langchain_community.vectorstores import Chroma\n",
      "    from langchain_openai import OpenAIEmbeddings\n",
      "    from langchain.chains import RetrievalQA\n",
      "    import os\n",
      "\n",
      "    os.environ[\"OPENAI_API_KEY\"] = \"sua_chave_openai_aqui\"\n",
      "\n",
      "    # Carregar o banco de dados vetorial persistente\n",
      "    embeddings = OpenAIEmbeddings()\n",
      "    vector_db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
      "\n",
      "    # Inicializar o LLM\n",
      "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # temperature=0 para respostas mais diretas\n",
      "\n",
      "    # Criar o Retriever (componente que busca no vector_db)\n",
      "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 3}) # k=3 busca os 3 chunks mais relevantes\n",
      "\n",
      "    # Criar a cadeia RAG\n",
      "    qa_chain = RetrievalQA.from_chain_type(\n",
      "        llm=llm,\n",
      "        chain_type=\"stuff\", # 'stuff' junta todos os chunks em um √∫nico prompt\n",
      "        retriever=retriever,\n",
      "        return_source_documents=True # Opcional: retorna os documentos fonte\n",
      "    )\n",
      "\n",
      "    # Fazer uma pergunta\n",
      "    query = \"Qual √© a pol√≠tica de f√©rias da empresa?\"\n",
      "    result = qa_chain.invoke({\"query\": query})\n",
      "\n",
      "    print(\"Resposta:\", result[\"result\"])\n",
      "    if \"source_documents\" in result:\n",
      "        print(\"\\nFontes:\")\n",
      "        for doc in result[\"source_documents\"]:\n",
      "            print(f\"- {doc.metadata.get('source', 'N/A')} (P√°gina: {doc.metadata.get('page', 'N/A')})\")\n",
      "\n",
      "    ```\n",
      "\n",
      "---\n",
      "\n",
      "### Considera√ß√µes Importantes e Melhores Pr√°ticas\n",
      "\n",
      "*   **Qualidade dos Dados:** A base de tudo. Dados ruins = respostas ruins.\n",
      "*   **Estrat√©gia de Chunking:**\n",
      "    *   **Tamanho do Chunk:** Pequenos demais podem perder contexto. Grandes demais podem exceder o limite do LLM ou incluir informa√ß√µes irrelevantes. Experimente!\n",
      "    *   **Overlap (Sobreposi√ß√£o):** Ter uma pequena sobreposi√ß√£o entre chunks ajuda a manter o contexto quando uma informa√ß√£o importante est√° na fronteira de dois chunks.\n",
      "*   **Modelo de Embedding:** A escolha do modelo afeta diretamente a relev√¢ncia da busca. Modelos mais novos e maiores geralmente s√£o melhores.\n",
      "*   **N√∫mero de Chunks Recuperados (`k`):** Se `k` for muito pequeno, pode perder informa√ß√µes. Se for muito grande, pode introduzir ru√≠do ou exceder o limite de tokens do LLM.\n",
      "*   **Prompt Engineering:** A forma como voc√™ estrutura o prompt para o LLM (instru√ß√µes, contexto, pergunta) √© crucial para a qualidade da resposta.\n",
      "*   **Avalia√ß√£o:** Como voc√™ mede se seu sistema RAG est√° funcionando bem? M√©tricas como relev√¢ncia, precis√£o e completude da resposta.\n",
      "*   **Lat√™ncia e Custo:** A busca no banco de dados vetorial e a chamada ao LLM t√™m custos de tempo e dinheiro. Otimize onde puder.\n",
      "*   **Seguran√ßa e Privacidade:** Se estiver usando dados sens√≠veis, garanta que seu banco de dados vetorial e as chamadas ao LLM estejam seguros e em conformidade com as regulamenta√ß√µes.\n",
      "\n",
      "---\n",
      "\n",
      "### Exemplos de Casos de Uso para RAG\n",
      "\n",
      "*   **Atendimento ao Cliente:** Respostas r√°pidas e precisas baseadas em FAQs, manuais de produto e hist√≥rico de tickets.\n",
      "*   **Pesquisa Interna:** Funcion√°rios podem fazer perguntas sobre pol√≠ticas da empresa, documentos t√©cnicos, etc.\n",
      "*   **Educa√ß√£o:** Alunos podem obter explica√ß√µes detalhadas de materiais de curso espec√≠ficos.\n",
      "*   **An√°lise Jur√≠dica/M√©dica:** Resumir e responder perguntas sobre grandes volumes de documentos legais ou artigos cient√≠ficos.\n",
      "*   **Gera√ß√£o de Conte√∫do:** Criar artigos, relat√≥rios ou resumos baseados em fontes espec√≠ficas.\n",
      "\n",
      "---\n",
      "\n",
      "RAG √© uma das t√©cnicas mais eficazes e amplamente adotadas para tornar os LLMs mais √∫teis e confi√°veis em aplica√ß√µes do mundo real. Comece com um conjunto de dados pequeno e ferramentas como LangChain/LlamaIndex e ChromaDB para entender o fluxo, e depois escale conforme suas necessidades.\n"
     ]
    }
   ],
   "source": [
    "resposta  = llm.invoke(\"Como usar RAG (de IA)? O que eu precisso saber?\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11244,
     "status": "ok",
     "timestamp": 1757625320484,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "uQx26aINnkzb",
    "outputId": "73725964-267f-42a5-cffb-5f5ddbcaf72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oi! Imagina que voc√™ tem um amigo muito, muito esperto, tipo um rob√¥ super inteligente, mas ele n√£o sabe *tudo* ainda.\n",
      "\n",
      "**Normalmente, para aprender algo novo,** ele precisa ver um mont√£o de exemplos. Tipo, se voc√™ quer que ele saiba o que √© um cachorro, voc√™ mostra 100 fotos de cachorros diferentes: um grande, um pequeno, um peludo, um sem pelo... e a√≠ ele aprende.\n",
      "\n",
      "**Mas o Few-shot √© diferente!** √â como se fosse uma m√°gica para aprender super r√°pido!\n",
      "\n",
      "Imagina que eu te mostro um bichinho de pel√∫cia que voc√™ nunca viu na vida. Eu digo: \"Olha, esse aqui √© um **Fuzzy-Wuzzy**!\"\n",
      "\n",
      "*   Eu te mostro **UM** Fuzzy-Wuzzy. (S√≥ um!)\n",
      "*   A√≠ eu te mostro **OUTRO** Fuzzy-Wuzzy, um pouquinho diferente, e digo: \"Esse tamb√©m √© um Fuzzy-Wuzzy!\" (Agora voc√™ viu **DOIS**!)\n",
      "\n",
      "E pronto! S√≥ com esses dois exemplos, se eu te mostrar uma foto de um monte de bichinhos e perguntar: \"Qual desses √© um Fuzzy-Wuzzy?\", voc√™ provavelmente vai saber!\n",
      "\n",
      "**Voc√™ aprendeu o que √© um Fuzzy-Wuzzy com *pouquinhos* exemplos!** N√£o precisei te mostrar 100 Fuzzy-Wuzzies. S√≥ um ou dois j√° foram o suficiente para voc√™ entender a ideia.\n",
      "\n",
      "**√â isso que o Few-shot faz com os computadores!** A gente mostra s√≥ um pouquinho de exemplos de uma coisa nova (tipo 1, 2 ou 3) e o computador j√° fica esperto o suficiente para reconhecer aquilo depois.\n",
      "\n",
      "√â como se ele fosse um detetive que precisa de poucas pistas para resolver um mist√©rio! Legal, n√©? üòâ\n"
     ]
    }
   ],
   "source": [
    "resposta  = llm.invoke(\"Me explique como se eu tivesse 8 anos, como se pode dar exemplos usando Few-shot\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21392,
     "status": "ok",
     "timestamp": 1757625773336,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "1RsDpCE4o5tY",
    "outputId": "310f4d0a-2361-478e-aec1-05e18214f6c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com certeza! O Few-shot learning √© uma t√©cnica poderosa para guiar um agente de IA com poucos exemplos, sem a necessidade de um treinamento massivo.\n",
      "\n",
      "Vamos criar um exemplo para o seu agente de IA aprender a encontrar c√¢meras online do Rio Jaguar√£o.\n",
      "\n",
      "---\n",
      "\n",
      "**Contexto:** Voc√™ tem um agente de IA baseado em um Large Language Model (LLM) que pode processar texto, realizar buscas na internet e extrair informa√ß√µes.\n",
      "\n",
      "**Objetivo:** Ensinar o agente a encontrar c√¢meras online do Rio Jaguar√£o, na fronteira com o Uruguai, com o prop√≥sito de verificar o n√≠vel das √°guas.\n",
      "\n",
      "---\n",
      "\n",
      "### Exemplo de Few-Shot Prompt para seu Agente de IA\n",
      "\n",
      "```\n",
      "// INSTRU√á√ÉO DO SISTEMA (System Prompt)\n",
      "// Esta parte define o papel e as capacidades do seu agente.\n",
      "Voc√™ √© um agente de IA especializado em encontrar c√¢meras online (webcams, streams, etc.) que mostrem rios espec√≠ficos, com foco em monitoramento de n√≠vel de √°gua ou condi√ß√µes gerais. Sua tarefa √© pesquisar na internet e fornecer links diretos ou descri√ß√µes de como acessar essas c√¢meras, priorizando fontes oficiais, de not√≠cias ou de monitoramento ambiental. Se n√£o encontrar c√¢meras diretas, sugira alternativas ou indique que a busca foi realizada mas sem sucesso aparente.\n",
      "\n",
      "// EXEMPLOS DE FEW-SHOT (Input-Output Pairs)\n",
      "\n",
      "// Exemplo 1: Rio Tiet√™ (foco em tr√°fego/condi√ß√µes gerais)\n",
      "User: Encontre c√¢meras online do Rio Tiet√™ em S√£o Paulo para ver o tr√°fego de barcos.\n",
      "Agent:\n",
      "A√ß√£o: Pesquisar por \"c√¢mera online Rio Tiet√™ S√£o Paulo tr√°fego barcos\" ou \"webcam Rio Tiet√™ navega√ß√£o\".\n",
      "Resultado Esperado:\n",
      "- **C√¢mera 1:** C√¢mera da CETESB monitorando trecho do Rio Tiet√™ (pode n√£o ser ao vivo 24h, mas oferece imagens recentes). [Link hipot√©tico: `https://cetesb.sp.gov.br/qualidade-do-ar/monitoramento/cameras/rio-tiete/`]\n",
      "- **C√¢mera 2:** Webcams de tr√°fego em pontes sobre o Tiet√™ que podem mostrar o rio. Ex: Ponte das Bandeiras. [Link hipot√©tico: `https://www.camerasonline.com.br/sp/tiete/`]\n",
      "- **Observa√ß√£o:** A maioria das c√¢meras focadas em tr√°fego rodovi√°rio pode oferecer uma vis√£o lateral do rio, mas n√£o um foco direto no n√≠vel da √°gua.\n",
      "\n",
      "// Exemplo 2: Rio Amazonas (foco em n√≠vel de √°gua/cheia)\n",
      "User: Procure webcams do Rio Amazonas perto de Manaus para monitorar a cheia.\n",
      "Agent:\n",
      "A√ß√£o: Pesquisar por \"webcam Rio Amazonas Manaus n√≠vel √°gua\" ou \"c√¢mera ao vivo porto Manaus rio\".\n",
      "Resultado Esperado:\n",
      "- **C√¢mera 1:** Porto de Manaus (muitas vezes possui c√¢meras que mostram o n√≠vel do rio e movimenta√ß√£o). [Link hipot√©tico: `https://www.portodemanaus.com.br/webcam/`]\n",
      "- **C√¢mera 2:** Canais de not√≠cias locais ou sites de turismo que ocasionalmente transmitem ao vivo durante per√≠odos de cheia. [Ex: `https://g1.globo.com/am/amazonas/` (buscar por 'ao vivo rio amazonas')]\n",
      "- **Observa√ß√£o:** A disponibilidade de c√¢meras focadas *exclusivamente* no n√≠vel da √°gua pode ser limitada, mas c√¢meras portu√°rias s√£o boas alternativas.\n",
      "\n",
      "// Exemplo 3: Rio Gua√≠ba (foco em n√≠vel de √°gua/monitoramento)\n",
      "User: Quero ver o n√≠vel da √°gua do Rio Gua√≠ba em Porto Alegre.\n",
      "Agent:\n",
      "A√ß√£o: Pesquisar por \"c√¢mera Rio Gua√≠ba Porto Alegre n√≠vel √°gua\" ou \"webcam Porto Alegre enchente Gua√≠ba\".\n",
      "Resultado Esperado:\n",
      "- **C√¢mera 1:** C√¢meras da prefeitura ou √≥rg√£os de defesa civil que monitoram √°reas de risco ou o cais do porto. [Link hipot√©tico: `https://www.portoalegre.rs.gov.br/defesacivil/cameras/`]\n",
      "- **C√¢mera 2:** Webcams de pontos tur√≠sticos com vista para o Gua√≠ba (ex: Orla do Gua√≠ba, Gas√¥metro). [Link hipot√©tico: `https://www.climaaovivo.com.br/rs/porto-alegre/orla-do-guaiba`]\n",
      "- **Observa√ß√£o:** Verificar a data/hora das imagens para garantir que s√£o recentes e relevantes para o n√≠vel da √°gua.\n",
      "\n",
      "// NOVA SOLICITA√á√ÉO (O seu pedido real para o agente)\n",
      "User: Encontre c√¢meras online que tenham vis√£o para o Rio Jaguar√£o, na fronteira com Uruguai, para que eu possa ter uma no√ß√£o do n√≠vel das √°guas.\n",
      "Agent:\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Como o Agente de IA (LLM) processaria isso:\n",
      "\n",
      "1.  **Compreens√£o da Tarefa (System Prompt):** O agente entende que seu papel √© encontrar c√¢meras de rios para monitoramento de √°gua, buscando links e priorizando fontes confi√°veis.\n",
      "\n",
      "2.  **An√°lise dos Exemplos (Few-Shot):**\n",
      "    *   Ele observa que os exemplos mostram como pesquisar (palavras-chave como \"c√¢mera online\", \"webcam\", \"n√≠vel √°gua\", \"cheia\", \"porto\").\n",
      "    *   Ele aprende a estrutura da resposta esperada: `A√ß√£o:` (o que ele faria para pesquisar) e `Resultado Esperado:` (links ou descri√ß√µes de c√¢meras, com observa√ß√µes).\n",
      "    *   Ele percebe a import√¢ncia de incluir o nome do rio, a cidade/regi√£o e o prop√≥sito (n√≠vel da √°gua, tr√°fego, etc.) nas buscas.\n",
      "    *   Ele entende que nem sempre haver√° uma c√¢mera *diretamente* focada no n√≠vel da √°gua, e que c√¢meras de portos, pontes ou pontos tur√≠sticos pr√≥ximos podem ser alternativas v√°lidas.\n",
      "    *   Ele aprende a lidar com a fronteira (Rio Jaguar√£o na fronteira com Uruguai) como um detalhe geogr√°fico importante.\n",
      "\n",
      "3.  **Aplica√ß√£o √† Nova Solicita√ß√£o:**\n",
      "    *   O agente pegaria a nova solicita√ß√£o: \"Encontre c√¢meras online que tenham vis√£o para o Rio Jaguar√£o, na fronteira com Uruguai, para que eu possa ter uma no√ß√£o do n√≠vel das √°guas.\"\n",
      "    *   Ele geraria uma resposta seguindo o padr√£o aprendido:\n",
      "\n",
      "    ```\n",
      "    // Resposta esperada do Agente de IA para a NOVA SOLICITA√á√ÉO\n",
      "    Agent:\n",
      "    A√ß√£o: Pesquisar por \"c√¢mera online Rio Jaguar√£o n√≠vel √°gua\", \"webcam Jaguar√£o fronteira Uruguai\", \"c√¢mera Porto Jaguar√£o\", \"webcam Rio Branco Uruguai Jaguar√£o\".\n",
      "    Resultado Esperado:\n",
      "    - **C√¢mera 1 (Potencial):** C√¢meras de √≥rg√£os governamentais ou de monitoramento ambiental do Brasil (ex: ANA, Defesa Civil RS) ou do Uruguai (ex: DINAMA, Prefectura Nacional Naval) que possam ter esta√ß√µes de monitoramento ou webcams na regi√£o de Jaguar√£o/Rio Branco. [Link hipot√©tico: `https://www.ana.gov.br/sala-de-situacao/` (buscar por esta√ß√£o Jaguar√£o) ou `https://www.mades.gub.uy/` (buscar por monitoramento h√≠drico)]\n",
      "    - **C√¢mera 2 (Potencial):** Webcams de cidades fronteiri√ßas como Jaguar√£o (Brasil) ou Rio Branco (Uruguai) que possam ter vista para o rio, especialmente em √°reas portu√°rias ou de pontes. [Link hipot√©tico: `https://www.climaaovivo.com.br/rs/jaguar√£o` (verificar se h√° c√¢meras com vista para o rio) ou sites de prefeituras locais.]\n",
      "    - **C√¢mera 3 (Not√≠cias/Turismo):** Sites de not√≠cias locais ou portais de turismo da regi√£o que ocasionalmente transmitem ao vivo ou publicam imagens atualizadas do rio, especialmente em per√≠odos de interesse (seca/cheia). [Ex: `https://g1.globo.com/rs/` (buscar por 'Rio Jaguar√£o ao vivo')]\n",
      "    - **Observa√ß√£o:** A disponibilidade de c√¢meras dedicadas ao n√≠vel da √°gua em rios de fronteira pode ser mais limitada. A busca deve focar em pontos estrat√©gicos como pontes internacionais (Ponte Mau√°) ou √°reas portu√°rias. Se n√£o houver c√¢meras ao vivo, dados de esta√ß√µes hidrom√©tricas podem ser a melhor alternativa para o n√≠vel da √°gua.\n",
      "    ```\n",
      "\n",
      "---\n",
      "\n",
      "Este exemplo mostra como, com apenas alguns pares de entrada/sa√≠da, voc√™ pode \"ensinar\" seu agente de IA a abordar um novo problema de forma estruturada e inteligente, mesmo que ele nunca tenha ouvido falar do Rio Jaguar√£o antes. Ele aprende o *padr√£o* de busca e resposta.\n"
     ]
    }
   ],
   "source": [
    "resposta  = llm.invoke(\"Exemplifique pra mim, um exemplo usando Few-shot, para que meu agente de IA aprenda a descobrir \" \\\n",
    "                       \"c√¢meras online que tenham vis√£o para o Rio Jaguar√£o, somente online, na qual se possa ter uma \" \\\n",
    "                       \"no√ß√£o do nivel das √°guas do Rio Jaguar√£o, na fronteira com Uruguai\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7028,
     "status": "ok",
     "timestamp": 1757639486320,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "Y6b-cX9reBE1",
    "outputId": "3704c86a-d976-4efb-de5e-2ffc4080daef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim, **exatamente!**\n",
      "\n",
      "**RAG** √© a abrevia√ß√£o (o acr√¥nimo) de **Retrieval-Augmented Generation**.\n",
      "\n",
      "S√£o a mesma coisa. \"RAG\" √© a forma curta e comumente usada para se referir √† t√©cnica completa de \"Retrieval-Augmented Generation\".\n",
      "\n",
      "√â como dizer \"IA\" para \"Intelig√™ncia Artificial\" ou \"LLM\" para \"Large Language Model\". A abrevia√ß√£o √© a forma mais pr√°tica e difundida de se referir ao conceito.\n"
     ]
    }
   ],
   "source": [
    "resposta  = llm.invoke(\"Ent√£o RAG e 'Retrieval-Argumented Generation' s√£o a mesma coisa (um √© abrevia√ß√£o da outra) ?\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13792,
     "status": "ok",
     "timestamp": 1757639808162,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "XWmbRQISeyeE",
    "outputId": "2d6b9361-5d70-4bef-d22d-00a1b3c6e786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamos simplificar isso com uma analogia!\n",
      "\n",
      "Imagine que voc√™ est√° conversando com uma pessoa.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Prompt do Usu√°rio (ou Prompt)\n",
      "\n",
      "*   **O que √©:** √â **o que voc√™ digita** para a intelig√™ncia artificial. √â a sua pergunta, o seu pedido, a sua instru√ß√£o direta.\n",
      "*   **Fun√ß√£o:** Dizer √† IA *o que* voc√™ quer que ela fa√ßa ou responda.\n",
      "*   **Analogia:** √â como **voc√™ falando diretamente com a pessoa**.\n",
      "    *   *Exemplo:* \"Ol√°, voc√™ pode me dar 5 ideias de nomes para um cachorro?\"\n",
      "    *   *Exemplo:* \"Me explique a teoria da relatividade de forma simples.\"\n",
      "    *   *Exemplo:* \"Escreva um e-mail de agradecimento para um cliente.\"\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Prompt do Sistema\n",
      "\n",
      "*   **O que √©:** √â uma **instru√ß√£o \"invis√≠vel\"** que √© dada √† intelig√™ncia artificial *antes* que ela veja o seu prompt. √â como um \"manual de instru√ß√µes\" ou um \"briefing\" para a IA sobre como ela deve se comportar.\n",
      "*   **Fun√ß√£o:** Definir o \"papel\", a \"personalidade\", o \"tom\", as \"regras\" ou as \"restri√ß√µes\" que a IA deve seguir ao responder. Ele molda *como* a IA vai interagir e responder.\n",
      "*   **Analogia:** √â como **algu√©m dando instru√ß√µes √† pessoa *antes* de voc√™ come√ßar a falar com ela**.\n",
      "    *   *Exemplo (instru√ß√£o para a pessoa):* \"Voc√™ √© um veterin√°rio experiente e amig√°vel. Sempre d√™ conselhos pr√°ticos e evite termos muito t√©cnicos. Seja emp√°tico com os donos dos animais.\"\n",
      "    *   *Exemplo (instru√ß√£o para a IA):* \"Voc√™ √© um professor de f√≠sica muito paciente e did√°tico. Use analogias do dia a dia e evite jarg√µes t√©cnicos.\"\n",
      "    *   *Exemplo (instru√ß√£o para a IA):* \"Voc√™ √© um assistente de marketing digital. Responda de forma concisa, use bullet points e sempre inclua uma chamada para a√ß√£o.\"\n",
      "\n",
      "---\n",
      "\n",
      "### Resumo da Diferen√ßa:\n",
      "\n",
      "| Caracter√≠stica      | Prompt do Usu√°rio (ou Prompt)                               | Prompt do Sistema                                                              |\n",
      "| :------------------ | :---------------------------------------------------------- | :----------------------------------------------------------------------------- |\n",
      "| **Quem digita?**    | Voc√™ (o usu√°rio)                                            | Geralmente o desenvolvedor da IA ou quem a est√° configurando para um uso       |\n",
      "| **O que √©?**        | Sua pergunta/pedido direto                                  | Instru√ß√µes sobre como a IA deve se comportar/responder                         |\n",
      "| **Visibilidade?**   | Vis√≠vel para voc√™                                           | Invis√≠vel para voc√™ (o usu√°rio final)                                          |\n",
      "| **Foco principal?** | *O que* fazer/responder                                     | *Como* fazer/responder (personalidade, tom, regras, contexto)                  |\n",
      "| **Analogia**        | **Voc√™ falando com a pessoa**                              | **Instru√ß√µes dadas √† pessoa *antes* de voc√™ falar com ela**                   |\n",
      "\n",
      "Em ess√™ncia, o **Prompt do Usu√°rio** √© o que voc√™ pede, e o **Prompt do Sistema** √© o que define a \"personalidade\" e as \"regras\" da IA para atender ao seu pedido. O Prompt do Sistema ajuda a garantir que a IA responda de forma consistente, √∫til e alinhada com o que seus criadores ou configuradores esperam dela.\n"
     ]
    }
   ],
   "source": [
    "resposta  = llm.invoke(\"Me explique de forma simples, a diferen√ßa de Prompt (ou Prompt do usu√°rio), e Prompt do Sistema\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAGEM_PROMPT = (\n",
    "    \"Voc√™ √© um triador de Service Desk para pol√≠ticas internas da empresa Carraro Desenvolvimento. \"\n",
    "    \"Dada a mensagem do usu√°rio, retorne SOMENTE um JSON com:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
    "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
    "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
    "    \"}\\n\"\n",
    "    \"Regras:\\n\"\n",
    "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas pol√≠ticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a pol√≠tica de alimenta√ß√£o em viagens?\").\\n'\n",
    "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informa√ß√µes para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma pol√≠tica\", \"Tenho uma d√∫vida geral\").\\n'\n",
    "    '- **ABRIR_CHAMADO**: Pedidos de exce√ß√£o, libera√ß√£o, aprova√ß√£o ou acesso especial, ou quando o usu√°rio explicitamente pede para abrir um chamado (Ex: \"Quero exce√ß√£o para trabalhar 5 dias remoto.\", \"Solicito libera√ß√£o para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
    "    \"Analise a mensagem e decida a a√ß√£o mais apropriada.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List, Dict\n",
    "\n",
    "class TriagemOutput(BaseModel):\n",
    "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
    "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
    "    campos_faltantes: List[str] = Field(default_factory=list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_triagem = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature = 0,          # Criatividade das respostas (0 a 1) 0 √© mais preciso\n",
    "    api_key=gemini_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "triagem_chain = llm_triagem.with_structured_output(TriagemOutput)\n",
    "\n",
    "def triagem(mensagem: str) -> Dict:\n",
    "    saida: TriagemOutput = triagem_chain.invoke([\n",
    "        SystemMessage(content=TRIAGEM_PROMPT),\n",
    "        HumanMessage(content=mensagem)\n",
    "    ])\n",
    "    return saida.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testes = [\"Posso reembolsar a internet?\",\n",
    "          \"Quero mais 5 dias de trabalho remoto. Como Fa√ßo?\",\n",
    "          \"Quantas capivaras tem no rio pinheiros?\"\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensagem: Posso reembolsar a internet?\n",
      "Resultado: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
      "\n",
      "Mensagem: Quero mais 5 dias de trabalho remoto. Como Fa√ßo?\n",
      "Resultado: {'decisao': 'ABRIR_CHAMADO', 'urgencia': 'MEDIA', 'campos_faltantes': []}\n",
      "\n",
      "Mensagem: Quantas capivaras tem no rio pinheiros?\n",
      "Resultado: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': ['contexto_da_politica']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for msg in testes:\n",
    "    resultado = triagem(msg)\n",
    "    print(f\"Mensagem: {msg}\\nResultado: {resultado}\\n\") "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO9RBpHgmliZY03v+u7mjcU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "google-colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
