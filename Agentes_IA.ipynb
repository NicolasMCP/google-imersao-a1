{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASVpw-Y6MxZf"
   },
   "source": [
    "# Aula 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Nota: \n",
    "$ uv init google-colab\n",
    "$ cd google-colab/\n",
    "$ uv venv\n",
    "$ source .venv/bin/activate\n",
    "$ code .\n",
    "$ uv pip install langchain-google-genai google-generativeai\n",
    "$ uv pip install ipykernel\n",
    "$ uv pip install jupyter\n",
    "$ uv pip install python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Acessa a chave da variável de ambiente\n",
    "gemini_key = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1757624376075,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "bX8cUIvL81HX"
   },
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature = 0,          # Criatividade das respostas (0 a 1) 0 é mais preciso\n",
    "    api_key=gemini_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21180,
     "status": "ok",
     "timestamp": 1757624703449,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "3ed42Ob5jrhf",
    "outputId": "26ee31bb-07b0-4322-f3f4-ec89321ba957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Retrieval Augmented Generation) é uma técnica poderosa que combina a capacidade de geração de texto de um Large Language Model (LLM) com a capacidade de recuperar informações de uma base de conhecimento externa. Em termos simples, ele permite que um LLM \"consulte\" documentos específicos antes de responder a uma pergunta, em vez de depender apenas do conhecimento em que foi treinado.\n",
      "\n",
      "Vamos detalhar o que você precisa saber e como usá-lo.\n",
      "\n",
      "---\n",
      "\n",
      "### O que é RAG (Retrieval Augmented Generation)?\n",
      "\n",
      "**R**etrieval (Recuperação): O sistema busca informações relevantes em uma base de dados externa (seus documentos, artigos, FAQs, etc.).\n",
      "**A**ugmented (Aumentada): As informações recuperadas são adicionadas à sua pergunta original, \"aumentando\" o prompt.\n",
      "**G**eneration (Geração): O LLM usa este prompt aumentado (sua pergunta + contexto recuperado) para gerar uma resposta mais precisa e fundamentada.\n",
      "\n",
      "**Por que RAG é importante?**\n",
      "\n",
      "1.  **Reduz Alucinações:** LLMs podem \"inventar\" fatos. RAG os força a basear suas respostas em dados reais.\n",
      "2.  **Informação Atualizada:** LLMs são treinados até uma data específica. RAG permite que eles acessem informações mais recentes ou proprietárias.\n",
      "3.  **Conhecimento Específico/Privado:** Você pode usar seus próprios documentos internos, manuais, bases de dados, etc., que o LLM nunca viu.\n",
      "4.  **Transparência e Atribuição:** Muitas implementações de RAG podem mostrar as fontes de onde a informação foi recuperada.\n",
      "5.  **Custo-benefício:** É mais barato e rápido do que fazer fine-tuning de um LLM para cada novo conjunto de dados.\n",
      "\n",
      "---\n",
      "\n",
      "### Como o RAG Funciona (Passo a Passo Conceitual)\n",
      "\n",
      "Imagine que você tem uma biblioteca de livros e um assistente muito inteligente (o LLM), mas que só leu alguns livros.\n",
      "\n",
      "1.  **Preparação da Biblioteca (Indexação dos Dados):**\n",
      "    *   **Seus Documentos:** Você tem vários documentos (PDFs, Word, texto puro, páginas web, etc.).\n",
      "    *   **Quebra em Pedaços (Chunking):** Cada documento é dividido em pequenos \"pedaços\" ou \"chunks\" (parágrafos, seções, frases). Isso é crucial porque o LLM tem um limite de tokens que pode processar de uma vez.\n",
      "    *   **Vetorização (Embedding):** Cada \"chunk\" é transformado em um vetor numérico (uma lista de números) por um \"modelo de embedding\". Vetores semanticamente semelhantes (que significam coisas parecidas) ficam \"próximos\" no espaço vetorial.\n",
      "    *   **Banco de Dados Vetorial (Vector Database):** Esses vetores são armazenados em um banco de dados especial (como Pinecone, Weaviate, Chroma, FAISS, etc.), junto com o texto original do chunk.\n",
      "\n",
      "2.  **A Consulta (Quando o Usuário Pergunta):**\n",
      "    *   **Pergunta do Usuário:** Você faz uma pergunta (ex: \"Qual é a política de férias da empresa?\").\n",
      "    *   **Vetorização da Pergunta:** Sua pergunta também é transformada em um vetor pelo mesmo modelo de embedding.\n",
      "    *   **Busca por Similaridade:** O sistema compara o vetor da sua pergunta com todos os vetores no banco de dados vetorial para encontrar os \"chunks\" mais semanticamente semelhantes.\n",
      "    *   **Recuperação dos Chunks:** Os chunks mais relevantes (e seus textos originais) são recuperados.\n",
      "\n",
      "3.  **Geração da Resposta (LLM em Ação):**\n",
      "    *   **Construção do Prompt:** Os chunks recuperados são inseridos em um prompt junto com a sua pergunta original. Exemplo:\n",
      "        ```\n",
      "        \"Contexto:\n",
      "        [Chunk 1: 'A política de férias da empresa X permite 30 dias de férias anuais após 1 ano de serviço...']\n",
      "        [Chunk 2: 'As férias devem ser solicitadas com 60 dias de antecedência...']\n",
      "\n",
      "        Pergunta: Qual é a política de férias da empresa?\n",
      "        ```\n",
      "    *   **LLM Gera a Resposta:** O LLM recebe este prompt enriquecido e usa o contexto fornecido para gerar uma resposta precisa e relevante.\n",
      "\n",
      "---\n",
      "\n",
      "### O que Você Precisa Saber/Ter para Implementar RAG?\n",
      "\n",
      "1.  **Seus Dados (Base de Conhecimento):**\n",
      "    *   **Formato:** PDFs, TXT, DOCX, HTML, JSON, CSV, etc.\n",
      "    *   **Qualidade:** Dados limpos, bem estruturados e relevantes são cruciais. \"Garbage in, garbage out\" se aplica aqui.\n",
      "\n",
      "2.  **Ferramentas de Processamento de Texto:**\n",
      "    *   **Bibliotecas de Chunking:** Para dividir seus documentos em pedaços gerenciáveis. Exemplos: `LangChain`, `LlamaIndex` (ambos em Python) oferecem utilitários para isso.\n",
      "    *   **Loaders de Documentos:** Para ler diferentes formatos de arquivo (PDF, DOCX, etc.). Também oferecidos por `LangChain` e `LlamaIndex`.\n",
      "\n",
      "3.  **Modelos de Embedding:**\n",
      "    *   **Função:** Transformar texto em vetores numéricos.\n",
      "    *   **Onde encontrar:** OpenAI (text-embedding-ada-002, text-embedding-3-small/large), Cohere, Hugging Face (ex: `sentence-transformers`).\n",
      "    *   **Escolha:** Modelos maiores geralmente são melhores, mas mais caros/lentos.\n",
      "\n",
      "4.  **Banco de Dados Vetorial (Vector Database):**\n",
      "    *   **Função:** Armazenar e pesquisar eficientemente os vetores dos seus chunks.\n",
      "    *   **Opções:**\n",
      "        *   **Gerenciados (Cloud):** Pinecone, Weaviate, Qdrant, Milvus (também pode ser self-hosted).\n",
      "        *   **Locais/Em Memória:** ChromaDB, FAISS (para prototipagem ou datasets menores).\n",
      "    *   **Escolha:** Depende da escala, custo, facilidade de uso e recursos necessários.\n",
      "\n",
      "5.  **Large Language Model (LLM):**\n",
      "    *   **Função:** Gerar a resposta final.\n",
      "    *   **Opções:**\n",
      "        *   **APIs (Cloud):** OpenAI (GPT-3.5, GPT-4), Anthropic (Claude), Google (Gemini).\n",
      "        *   **Open Source (Self-hosted):** Llama 2, Mistral, Mixtral (exigem hardware potente para rodar localmente).\n",
      "    *   **Escolha:** Depende do custo, desempenho, privacidade e capacidade de hardware.\n",
      "\n",
      "6.  **Frameworks de Orquestração (Opcional, mas Altamente Recomendado):**\n",
      "    *   **LangChain (Python/JS):** Um dos mais populares. Facilita a construção de pipelines complexos, integrando loaders, chunkers, embeddings, vector dbs e LLMs.\n",
      "    *   **LlamaIndex (Python):** Focado especificamente em RAG e indexação de dados para LLMs. Ótimo para começar com RAG.\n",
      "    *   **Outros:** Semantic Kernel (Microsoft, .NET), Haystack (Deepset).\n",
      "\n",
      "7.  **Conhecimento de Programação:**\n",
      "    *   **Python:** É a linguagem dominante para IA e ML, e a maioria das bibliotecas e frameworks estão em Python.\n",
      "\n",
      "---\n",
      "\n",
      "### Como Usar RAG (Passos Práticos de Implementação)\n",
      "\n",
      "Vamos usar um exemplo com Python, LangChain e ChromaDB (para simplicidade).\n",
      "\n",
      "**Fase 1: Indexação (Uma Vez ou Periodicamente)**\n",
      "\n",
      "1.  **Instale as Bibliotecas:**\n",
      "    ```bash\n",
      "    pip install langchain openai chromadb pypdf\n",
      "    ```\n",
      "2.  **Carregue Seus Documentos:**\n",
      "    ```python\n",
      "    from langchain_community.document_loaders import PyPDFLoader\n",
      "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
      "    from langchain_openai import OpenAIEmbeddings\n",
      "    from langchain_community.vectorstores import Chroma\n",
      "    import os\n",
      "\n",
      "    # Configure sua chave da OpenAI\n",
      "    os.environ[\"OPENAI_API_KEY\"] = \"sua_chave_openai_aqui\"\n",
      "\n",
      "    # 1. Carregar Documentos (ex: um PDF)\n",
      "    loader = PyPDFLoader(\"seus_documentos/politica_da_empresa.pdf\")\n",
      "    documents = loader.load()\n",
      "\n",
      "    # 2. Dividir em Chunks\n",
      "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
      "    chunks = text_splitter.split_documents(documents)\n",
      "\n",
      "    # 3. Gerar Embeddings\n",
      "    embeddings = OpenAIEmbeddings()\n",
      "\n",
      "    # 4. Armazenar no Vector Database (ChromaDB neste exemplo)\n",
      "    # Isso cria um diretório 'chroma_db' para armazenar os embeddings\n",
      "    vector_db = Chroma.from_documents(\n",
      "        documents=chunks,\n",
      "        embedding=embeddings,\n",
      "        persist_directory=\"./chroma_db\"\n",
      "    )\n",
      "    vector_db.persist() # Salva o banco de dados no disco\n",
      "    print(\"Documentos indexados com sucesso!\")\n",
      "    ```\n",
      "\n",
      "**Fase 2: Consulta (Quando o Usuário Faz uma Pergunta)**\n",
      "\n",
      "1.  **Carregue o Vector Database e o LLM:**\n",
      "    ```python\n",
      "    from langchain_openai import ChatOpenAI\n",
      "    from langchain_community.vectorstores import Chroma\n",
      "    from langchain_openai import OpenAIEmbeddings\n",
      "    from langchain.chains import RetrievalQA\n",
      "    import os\n",
      "\n",
      "    os.environ[\"OPENAI_API_KEY\"] = \"sua_chave_openai_aqui\"\n",
      "\n",
      "    # Carregar o banco de dados vetorial persistente\n",
      "    embeddings = OpenAIEmbeddings()\n",
      "    vector_db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
      "\n",
      "    # Inicializar o LLM\n",
      "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # temperature=0 para respostas mais diretas\n",
      "\n",
      "    # Criar o Retriever (componente que busca no vector_db)\n",
      "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 3}) # k=3 busca os 3 chunks mais relevantes\n",
      "\n",
      "    # Criar a cadeia RAG\n",
      "    qa_chain = RetrievalQA.from_chain_type(\n",
      "        llm=llm,\n",
      "        chain_type=\"stuff\", # 'stuff' junta todos os chunks em um único prompt\n",
      "        retriever=retriever,\n",
      "        return_source_documents=True # Opcional: retorna os documentos fonte\n",
      "    )\n",
      "\n",
      "    # Fazer uma pergunta\n",
      "    query = \"Qual é a política de férias da empresa?\"\n",
      "    result = qa_chain.invoke({\"query\": query})\n",
      "\n",
      "    print(\"Resposta:\", result[\"result\"])\n",
      "    if \"source_documents\" in result:\n",
      "        print(\"\\nFontes:\")\n",
      "        for doc in result[\"source_documents\"]:\n",
      "            print(f\"- {doc.metadata.get('source', 'N/A')} (Página: {doc.metadata.get('page', 'N/A')})\")\n",
      "\n",
      "    ```\n",
      "\n",
      "---\n",
      "\n",
      "### Considerações Importantes e Melhores Práticas\n",
      "\n",
      "*   **Qualidade dos Dados:** A base de tudo. Dados ruins = respostas ruins.\n",
      "*   **Estratégia de Chunking:**\n",
      "    *   **Tamanho do Chunk:** Pequenos demais podem perder contexto. Grandes demais podem exceder o limite do LLM ou incluir informações irrelevantes. Experimente!\n",
      "    *   **Overlap (Sobreposição):** Ter uma pequena sobreposição entre chunks ajuda a manter o contexto quando uma informação importante está na fronteira de dois chunks.\n",
      "*   **Modelo de Embedding:** A escolha do modelo afeta diretamente a relevância da busca. Modelos mais novos e maiores geralmente são melhores.\n",
      "*   **Número de Chunks Recuperados (`k`):** Se `k` for muito pequeno, pode perder informações. Se for muito grande, pode introduzir ruído ou exceder o limite de tokens do LLM.\n",
      "*   **Prompt Engineering:** A forma como você estrutura o prompt para o LLM (instruções, contexto, pergunta) é crucial para a qualidade da resposta.\n",
      "*   **Avaliação:** Como você mede se seu sistema RAG está funcionando bem? Métricas como relevância, precisão e completude da resposta.\n",
      "*   **Latência e Custo:** A busca no banco de dados vetorial e a chamada ao LLM têm custos de tempo e dinheiro. Otimize onde puder.\n",
      "*   **Segurança e Privacidade:** Se estiver usando dados sensíveis, garanta que seu banco de dados vetorial e as chamadas ao LLM estejam seguros e em conformidade com as regulamentações.\n",
      "\n",
      "---\n",
      "\n",
      "### Exemplos de Casos de Uso para RAG\n",
      "\n",
      "*   **Atendimento ao Cliente:** Respostas rápidas e precisas baseadas em FAQs, manuais de produto e histórico de tickets.\n",
      "*   **Pesquisa Interna:** Funcionários podem fazer perguntas sobre políticas da empresa, documentos técnicos, etc.\n",
      "*   **Educação:** Alunos podem obter explicações detalhadas de materiais de curso específicos.\n",
      "*   **Análise Jurídica/Médica:** Resumir e responder perguntas sobre grandes volumes de documentos legais ou artigos científicos.\n",
      "*   **Geração de Conteúdo:** Criar artigos, relatórios ou resumos baseados em fontes específicas.\n",
      "\n",
      "---\n",
      "\n",
      "RAG é uma das técnicas mais eficazes e amplamente adotadas para tornar os LLMs mais úteis e confiáveis em aplicações do mundo real. Comece com um conjunto de dados pequeno e ferramentas como LangChain/LlamaIndex e ChromaDB para entender o fluxo, e depois escale conforme suas necessidades.\n"
     ]
    }
   ],
   "source": [
    "resposta  = llm.invoke(\"Como usar RAG (de IA)? O que eu precisso saber?\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11244,
     "status": "ok",
     "timestamp": 1757625320484,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "uQx26aINnkzb",
    "outputId": "73725964-267f-42a5-cffb-5f5ddbcaf72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oi! Imagina que você tem um amigo muito, muito esperto, tipo um robô super inteligente, mas ele não sabe *tudo* ainda.\n",
      "\n",
      "**Normalmente, para aprender algo novo,** ele precisa ver um montão de exemplos. Tipo, se você quer que ele saiba o que é um cachorro, você mostra 100 fotos de cachorros diferentes: um grande, um pequeno, um peludo, um sem pelo... e aí ele aprende.\n",
      "\n",
      "**Mas o Few-shot é diferente!** É como se fosse uma mágica para aprender super rápido!\n",
      "\n",
      "Imagina que eu te mostro um bichinho de pelúcia que você nunca viu na vida. Eu digo: \"Olha, esse aqui é um **Fuzzy-Wuzzy**!\"\n",
      "\n",
      "*   Eu te mostro **UM** Fuzzy-Wuzzy. (Só um!)\n",
      "*   Aí eu te mostro **OUTRO** Fuzzy-Wuzzy, um pouquinho diferente, e digo: \"Esse também é um Fuzzy-Wuzzy!\" (Agora você viu **DOIS**!)\n",
      "\n",
      "E pronto! Só com esses dois exemplos, se eu te mostrar uma foto de um monte de bichinhos e perguntar: \"Qual desses é um Fuzzy-Wuzzy?\", você provavelmente vai saber!\n",
      "\n",
      "**Você aprendeu o que é um Fuzzy-Wuzzy com *pouquinhos* exemplos!** Não precisei te mostrar 100 Fuzzy-Wuzzies. Só um ou dois já foram o suficiente para você entender a ideia.\n",
      "\n",
      "**É isso que o Few-shot faz com os computadores!** A gente mostra só um pouquinho de exemplos de uma coisa nova (tipo 1, 2 ou 3) e o computador já fica esperto o suficiente para reconhecer aquilo depois.\n",
      "\n",
      "É como se ele fosse um detetive que precisa de poucas pistas para resolver um mistério! Legal, né? 😉\n"
     ]
    }
   ],
   "source": [
    "resposta  = llm.invoke(\"Me explique como se eu tivesse 8 anos, como se pode dar exemplos usando Few-shot\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21392,
     "status": "ok",
     "timestamp": 1757625773336,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "1RsDpCE4o5tY",
    "outputId": "310f4d0a-2361-478e-aec1-05e18214f6c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com certeza! O Few-shot learning é uma técnica poderosa para guiar um agente de IA com poucos exemplos, sem a necessidade de um treinamento massivo.\n",
      "\n",
      "Vamos criar um exemplo para o seu agente de IA aprender a encontrar câmeras online do Rio Jaguarão.\n",
      "\n",
      "---\n",
      "\n",
      "**Contexto:** Você tem um agente de IA baseado em um Large Language Model (LLM) que pode processar texto, realizar buscas na internet e extrair informações.\n",
      "\n",
      "**Objetivo:** Ensinar o agente a encontrar câmeras online do Rio Jaguarão, na fronteira com o Uruguai, com o propósito de verificar o nível das águas.\n",
      "\n",
      "---\n",
      "\n",
      "### Exemplo de Few-Shot Prompt para seu Agente de IA\n",
      "\n",
      "```\n",
      "// INSTRUÇÃO DO SISTEMA (System Prompt)\n",
      "// Esta parte define o papel e as capacidades do seu agente.\n",
      "Você é um agente de IA especializado em encontrar câmeras online (webcams, streams, etc.) que mostrem rios específicos, com foco em monitoramento de nível de água ou condições gerais. Sua tarefa é pesquisar na internet e fornecer links diretos ou descrições de como acessar essas câmeras, priorizando fontes oficiais, de notícias ou de monitoramento ambiental. Se não encontrar câmeras diretas, sugira alternativas ou indique que a busca foi realizada mas sem sucesso aparente.\n",
      "\n",
      "// EXEMPLOS DE FEW-SHOT (Input-Output Pairs)\n",
      "\n",
      "// Exemplo 1: Rio Tietê (foco em tráfego/condições gerais)\n",
      "User: Encontre câmeras online do Rio Tietê em São Paulo para ver o tráfego de barcos.\n",
      "Agent:\n",
      "Ação: Pesquisar por \"câmera online Rio Tietê São Paulo tráfego barcos\" ou \"webcam Rio Tietê navegação\".\n",
      "Resultado Esperado:\n",
      "- **Câmera 1:** Câmera da CETESB monitorando trecho do Rio Tietê (pode não ser ao vivo 24h, mas oferece imagens recentes). [Link hipotético: `https://cetesb.sp.gov.br/qualidade-do-ar/monitoramento/cameras/rio-tiete/`]\n",
      "- **Câmera 2:** Webcams de tráfego em pontes sobre o Tietê que podem mostrar o rio. Ex: Ponte das Bandeiras. [Link hipotético: `https://www.camerasonline.com.br/sp/tiete/`]\n",
      "- **Observação:** A maioria das câmeras focadas em tráfego rodoviário pode oferecer uma visão lateral do rio, mas não um foco direto no nível da água.\n",
      "\n",
      "// Exemplo 2: Rio Amazonas (foco em nível de água/cheia)\n",
      "User: Procure webcams do Rio Amazonas perto de Manaus para monitorar a cheia.\n",
      "Agent:\n",
      "Ação: Pesquisar por \"webcam Rio Amazonas Manaus nível água\" ou \"câmera ao vivo porto Manaus rio\".\n",
      "Resultado Esperado:\n",
      "- **Câmera 1:** Porto de Manaus (muitas vezes possui câmeras que mostram o nível do rio e movimentação). [Link hipotético: `https://www.portodemanaus.com.br/webcam/`]\n",
      "- **Câmera 2:** Canais de notícias locais ou sites de turismo que ocasionalmente transmitem ao vivo durante períodos de cheia. [Ex: `https://g1.globo.com/am/amazonas/` (buscar por 'ao vivo rio amazonas')]\n",
      "- **Observação:** A disponibilidade de câmeras focadas *exclusivamente* no nível da água pode ser limitada, mas câmeras portuárias são boas alternativas.\n",
      "\n",
      "// Exemplo 3: Rio Guaíba (foco em nível de água/monitoramento)\n",
      "User: Quero ver o nível da água do Rio Guaíba em Porto Alegre.\n",
      "Agent:\n",
      "Ação: Pesquisar por \"câmera Rio Guaíba Porto Alegre nível água\" ou \"webcam Porto Alegre enchente Guaíba\".\n",
      "Resultado Esperado:\n",
      "- **Câmera 1:** Câmeras da prefeitura ou órgãos de defesa civil que monitoram áreas de risco ou o cais do porto. [Link hipotético: `https://www.portoalegre.rs.gov.br/defesacivil/cameras/`]\n",
      "- **Câmera 2:** Webcams de pontos turísticos com vista para o Guaíba (ex: Orla do Guaíba, Gasômetro). [Link hipotético: `https://www.climaaovivo.com.br/rs/porto-alegre/orla-do-guaiba`]\n",
      "- **Observação:** Verificar a data/hora das imagens para garantir que são recentes e relevantes para o nível da água.\n",
      "\n",
      "// NOVA SOLICITAÇÃO (O seu pedido real para o agente)\n",
      "User: Encontre câmeras online que tenham visão para o Rio Jaguarão, na fronteira com Uruguai, para que eu possa ter uma noção do nível das águas.\n",
      "Agent:\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Como o Agente de IA (LLM) processaria isso:\n",
      "\n",
      "1.  **Compreensão da Tarefa (System Prompt):** O agente entende que seu papel é encontrar câmeras de rios para monitoramento de água, buscando links e priorizando fontes confiáveis.\n",
      "\n",
      "2.  **Análise dos Exemplos (Few-Shot):**\n",
      "    *   Ele observa que os exemplos mostram como pesquisar (palavras-chave como \"câmera online\", \"webcam\", \"nível água\", \"cheia\", \"porto\").\n",
      "    *   Ele aprende a estrutura da resposta esperada: `Ação:` (o que ele faria para pesquisar) e `Resultado Esperado:` (links ou descrições de câmeras, com observações).\n",
      "    *   Ele percebe a importância de incluir o nome do rio, a cidade/região e o propósito (nível da água, tráfego, etc.) nas buscas.\n",
      "    *   Ele entende que nem sempre haverá uma câmera *diretamente* focada no nível da água, e que câmeras de portos, pontes ou pontos turísticos próximos podem ser alternativas válidas.\n",
      "    *   Ele aprende a lidar com a fronteira (Rio Jaguarão na fronteira com Uruguai) como um detalhe geográfico importante.\n",
      "\n",
      "3.  **Aplicação à Nova Solicitação:**\n",
      "    *   O agente pegaria a nova solicitação: \"Encontre câmeras online que tenham visão para o Rio Jaguarão, na fronteira com Uruguai, para que eu possa ter uma noção do nível das águas.\"\n",
      "    *   Ele geraria uma resposta seguindo o padrão aprendido:\n",
      "\n",
      "    ```\n",
      "    // Resposta esperada do Agente de IA para a NOVA SOLICITAÇÃO\n",
      "    Agent:\n",
      "    Ação: Pesquisar por \"câmera online Rio Jaguarão nível água\", \"webcam Jaguarão fronteira Uruguai\", \"câmera Porto Jaguarão\", \"webcam Rio Branco Uruguai Jaguarão\".\n",
      "    Resultado Esperado:\n",
      "    - **Câmera 1 (Potencial):** Câmeras de órgãos governamentais ou de monitoramento ambiental do Brasil (ex: ANA, Defesa Civil RS) ou do Uruguai (ex: DINAMA, Prefectura Nacional Naval) que possam ter estações de monitoramento ou webcams na região de Jaguarão/Rio Branco. [Link hipotético: `https://www.ana.gov.br/sala-de-situacao/` (buscar por estação Jaguarão) ou `https://www.mades.gub.uy/` (buscar por monitoramento hídrico)]\n",
      "    - **Câmera 2 (Potencial):** Webcams de cidades fronteiriças como Jaguarão (Brasil) ou Rio Branco (Uruguai) que possam ter vista para o rio, especialmente em áreas portuárias ou de pontes. [Link hipotético: `https://www.climaaovivo.com.br/rs/jaguarão` (verificar se há câmeras com vista para o rio) ou sites de prefeituras locais.]\n",
      "    - **Câmera 3 (Notícias/Turismo):** Sites de notícias locais ou portais de turismo da região que ocasionalmente transmitem ao vivo ou publicam imagens atualizadas do rio, especialmente em períodos de interesse (seca/cheia). [Ex: `https://g1.globo.com/rs/` (buscar por 'Rio Jaguarão ao vivo')]\n",
      "    - **Observação:** A disponibilidade de câmeras dedicadas ao nível da água em rios de fronteira pode ser mais limitada. A busca deve focar em pontos estratégicos como pontes internacionais (Ponte Mauá) ou áreas portuárias. Se não houver câmeras ao vivo, dados de estações hidrométricas podem ser a melhor alternativa para o nível da água.\n",
      "    ```\n",
      "\n",
      "---\n",
      "\n",
      "Este exemplo mostra como, com apenas alguns pares de entrada/saída, você pode \"ensinar\" seu agente de IA a abordar um novo problema de forma estruturada e inteligente, mesmo que ele nunca tenha ouvido falar do Rio Jaguarão antes. Ele aprende o *padrão* de busca e resposta.\n"
     ]
    }
   ],
   "source": [
    "resposta  = llm.invoke(\"Exemplifique pra mim, um exemplo usando Few-shot, para que meu agente de IA aprenda a descobrir \" \\\n",
    "                       \"câmeras online que tenham visão para o Rio Jaguarão, somente online, na qual se possa ter uma \" \\\n",
    "                       \"noção do nivel das águas do Rio Jaguarão, na fronteira com Uruguai\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7028,
     "status": "ok",
     "timestamp": 1757639486320,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "Y6b-cX9reBE1",
    "outputId": "3704c86a-d976-4efb-de5e-2ffc4080daef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim, **exatamente!**\n",
      "\n",
      "**RAG** é a abreviação (o acrônimo) de **Retrieval-Augmented Generation**.\n",
      "\n",
      "São a mesma coisa. \"RAG\" é a forma curta e comumente usada para se referir à técnica completa de \"Retrieval-Augmented Generation\".\n",
      "\n",
      "É como dizer \"IA\" para \"Inteligência Artificial\" ou \"LLM\" para \"Large Language Model\". A abreviação é a forma mais prática e difundida de se referir ao conceito.\n"
     ]
    }
   ],
   "source": [
    "resposta  = llm.invoke(\"Então RAG e 'Retrieval-Argumented Generation' são a mesma coisa (um é abreviação da outra) ?\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13792,
     "status": "ok",
     "timestamp": 1757639808162,
     "user": {
      "displayName": "Nicolas Ramos",
      "userId": "04329562268576061818"
     },
     "user_tz": 180
    },
    "id": "XWmbRQISeyeE",
    "outputId": "2d6b9361-5d70-4bef-d22d-00a1b3c6e786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamos simplificar isso com uma analogia!\n",
      "\n",
      "Imagine que você está conversando com uma pessoa.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Prompt do Usuário (ou Prompt)\n",
      "\n",
      "*   **O que é:** É **o que você digita** para a inteligência artificial. É a sua pergunta, o seu pedido, a sua instrução direta.\n",
      "*   **Função:** Dizer à IA *o que* você quer que ela faça ou responda.\n",
      "*   **Analogia:** É como **você falando diretamente com a pessoa**.\n",
      "    *   *Exemplo:* \"Olá, você pode me dar 5 ideias de nomes para um cachorro?\"\n",
      "    *   *Exemplo:* \"Me explique a teoria da relatividade de forma simples.\"\n",
      "    *   *Exemplo:* \"Escreva um e-mail de agradecimento para um cliente.\"\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Prompt do Sistema\n",
      "\n",
      "*   **O que é:** É uma **instrução \"invisível\"** que é dada à inteligência artificial *antes* que ela veja o seu prompt. É como um \"manual de instruções\" ou um \"briefing\" para a IA sobre como ela deve se comportar.\n",
      "*   **Função:** Definir o \"papel\", a \"personalidade\", o \"tom\", as \"regras\" ou as \"restrições\" que a IA deve seguir ao responder. Ele molda *como* a IA vai interagir e responder.\n",
      "*   **Analogia:** É como **alguém dando instruções à pessoa *antes* de você começar a falar com ela**.\n",
      "    *   *Exemplo (instrução para a pessoa):* \"Você é um veterinário experiente e amigável. Sempre dê conselhos práticos e evite termos muito técnicos. Seja empático com os donos dos animais.\"\n",
      "    *   *Exemplo (instrução para a IA):* \"Você é um professor de física muito paciente e didático. Use analogias do dia a dia e evite jargões técnicos.\"\n",
      "    *   *Exemplo (instrução para a IA):* \"Você é um assistente de marketing digital. Responda de forma concisa, use bullet points e sempre inclua uma chamada para ação.\"\n",
      "\n",
      "---\n",
      "\n",
      "### Resumo da Diferença:\n",
      "\n",
      "| Característica      | Prompt do Usuário (ou Prompt)                               | Prompt do Sistema                                                              |\n",
      "| :------------------ | :---------------------------------------------------------- | :----------------------------------------------------------------------------- |\n",
      "| **Quem digita?**    | Você (o usuário)                                            | Geralmente o desenvolvedor da IA ou quem a está configurando para um uso       |\n",
      "| **O que é?**        | Sua pergunta/pedido direto                                  | Instruções sobre como a IA deve se comportar/responder                         |\n",
      "| **Visibilidade?**   | Visível para você                                           | Invisível para você (o usuário final)                                          |\n",
      "| **Foco principal?** | *O que* fazer/responder                                     | *Como* fazer/responder (personalidade, tom, regras, contexto)                  |\n",
      "| **Analogia**        | **Você falando com a pessoa**                              | **Instruções dadas à pessoa *antes* de você falar com ela**                   |\n",
      "\n",
      "Em essência, o **Prompt do Usuário** é o que você pede, e o **Prompt do Sistema** é o que define a \"personalidade\" e as \"regras\" da IA para atender ao seu pedido. O Prompt do Sistema ajuda a garantir que a IA responda de forma consistente, útil e alinhada com o que seus criadores ou configuradores esperam dela.\n"
     ]
    }
   ],
   "source": [
    "resposta  = llm.invoke(\"Me explique de forma simples, a diferença de Prompt (ou Prompt do usuário), e Prompt do Sistema\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAGEM_PROMPT = (\n",
    "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
    "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
    "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
    "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
    "    \"}\\n\"\n",
    "    \"Regras:\\n\"\n",
    "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
    "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
    "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
    "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List, Dict\n",
    "\n",
    "class TriagemOutput(BaseModel):\n",
    "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
    "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
    "    campos_faltantes: List[str] = Field(default_factory=list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_triagem = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature = 0,          # Criatividade das respostas (0 a 1) 0 é mais preciso\n",
    "    api_key=gemini_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "triagem_chain = llm_triagem.with_structured_output(TriagemOutput)\n",
    "\n",
    "def triagem(mensagem: str) -> Dict:\n",
    "    saida: TriagemOutput = triagem_chain.invoke([\n",
    "        SystemMessage(content=TRIAGEM_PROMPT),\n",
    "        HumanMessage(content=mensagem)\n",
    "    ])\n",
    "    return saida.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testes = [\"Posso reembolsar a internet?\",\n",
    "          \"Quero mais 5 dias de trabalho remoto. Como Faço?\",\n",
    "          \"Quantas capivaras tem no rio pinheiros?\"\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensagem: Posso reembolsar a internet?\n",
      "Resultado: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
      "\n",
      "Mensagem: Quero mais 5 dias de trabalho remoto. Como Faço?\n",
      "Resultado: {'decisao': 'ABRIR_CHAMADO', 'urgencia': 'MEDIA', 'campos_faltantes': []}\n",
      "\n",
      "Mensagem: Quantas capivaras tem no rio pinheiros?\n",
      "Resultado: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': ['contexto_da_politica']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for msg in testes:\n",
    "    resultado = triagem(msg)\n",
    "    print(f\"Mensagem: {msg}\\nResultado: {resultado}\\n\") "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO9RBpHgmliZY03v+u7mjcU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "google-colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
